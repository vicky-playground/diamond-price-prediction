{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2e233f9d-09b2-427b-b199-bb2807562537",
      "metadata": {},
      "outputs": [],
      "source": [
        "# **Build an AI Web App for Diamond Price Prediction ðŸ’Ž**\n\nEstimated time needed: **90** minutes\n\nThis project is designed to help you understand the key steps in building machine learning models, evaluating their performance, and deploying the results using the open-source Mercury framework. The end result of this project will resemble the demo [the demo](https://ibm.runmercury.com/app/app-1) we've prepared.\n\n## Objectives\nBy the end of this hands-on project, you will be able to:\n- Master the data preprocessing pipeline, including feature selection, encoding, and handling outliers.\n- Explore and understand the concepts of correlation analysis and feature visualization.\n- Build and evaluate various machine learning models, interpreting metrics like MAE, MSE, and R-squared.\n- Identify and choose the most effective model for making future diamond price predictions.\n- Use open-source framework, Mercury, to share the exciting outcomes of your predictions with a global audience through the web.\n\nNow, let's dive into the project step by step:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65f10014-80ee-4055-ab03-aa6fe1b28427",
      "metadata": {},
      "outputs": [],
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3d449c6-bb53-4b0f-8dfc-b4966dd5d5ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33803c2c-7efa-4557-9c15-580d1ad324b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "For this project, we will be using the following libraries:\n\n*   `pandas` for managing the data.\n*   `numpy`for mathematical operations.\n*   `sklearn` for machine learning and machine-learning-pipeline related functions.\n*   `seaborn` for visualizing the data.\n*   `matplotlib` for additional plotting tools.\n*   `mercury` for building a web app."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2da196f2-c9be-4670-a880-2b3175d036e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Importing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "id": "0d397c93-41e4-4c6b-8956-75a769cd4636",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set(context=\"notebook\", palette=\"Spectral\", style = 'darkgrid' ,font_scale = 1.5, color_codes=False)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.linear_model import LinearRegression\n# Regression\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor \nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"numpy\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"sklearn\")\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"seaborn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e67c9f1-4070-4002-aa45-b60517a1cf8b",
      "metadata": {},
      "outputs": [],
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d80c6843-6605-4785-8dc5-593813aea3e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Data Loading and Preprocessing\nIn this section, we'll start by loading the diamond price dataset from a provided URL. We'll then preprocess the data to make it suitable for our machine learning tasks.\n\nTasks:\n\n1. Load the dataset into a pandas DataFrame.\n2. Data preprocessing including:\n    - Remove unnecessary columns like index and dimensions.\n    - Convert categorical features (cut, color, clarity) into numerical representations.\n    - Calculate correlation coefficients and visualize them using a heatmap.\n3. Feature visualization\n4. Data splits and handle outliers using Isolation Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2921c835-b2dd-4346-8710-427ee3a622c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Task 1: Load the Diamond Price Dataset\nThere are 10 attributes included in the dataset including the target ie. price.\n\nFeature description:\n- price price in US dollars (\\$326--\\$18,823)\n- carat weight of the diamond (0.2--5.01)\n- cut quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n- color diamond colour, from J (worst) to D (best)\n- clarity a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n- x length in mm (0--10.74)\n- y width in mm (0--58.9)\n- z depth in mm (0--31.8)\n- depth total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)\n- table width of top of diamond relative to widest point (43--95)"
      ]
    },
    {
      "cell_type": "code",
      "id": "132cf4f3-968d-41a3-805d-52f2cc6a4a8d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# URL for the diamond price dataset\nURL = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/diamonds.csv\"\n\n# Load the dataset into a DataFrame\ndf = pd.read_csv(URL)\n\ndf.head()"
      ]
    },
    {
      "cell_type": "code",
      "id": "26f95c17-7144-4aab-85e8-978fc2dd23d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\"\"The DataFrame has {df.shape[0]} rows and {df.shape[1]} columns.\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ea80406-35ef-40ba-8db3-c2173f13d1e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Task 2: Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e7bbfed-191c-4144-8d53-78ce53e64a64",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Remove Unnecessary Data\nThe first column seems to be just index, so we can remove it.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "0b66606d-e485-4956-968f-5647d0f205b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.drop([\"s\"], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcb185e0-f4ca-4f6d-b21f-9ad4ccc5f53f",
      "metadata": {},
      "outputs": [],
      "source": [
        "Since the size (length x width x depth) of the diamond have a high positive correlation with carat, which can indicate multicollinearity and affect model stability and interpretability. We can drop the features of x, y, z to prevent it."
      ]
    },
    {
      "cell_type": "code",
      "id": "fae598a0-1690-4a43-8ba3-215fc74153b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.drop([\"x\",\"y\",'z'], axis=1)\ndf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef8afe0b-5192-4c91-b7f4-bea46e317496",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Convert categorical features (e.g., 'cut', 'color', 'clarity') to numerical"
      ]
    },
    {
      "cell_type": "code",
      "id": "aebcf3e6-63ec-4666-9a85-e413d3459b47",
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"cut\"] = df[\"cut\"].map({\"Ideal\": 1, \"Premium\": 2, \"Good\": 3, \"Very Good\": 4, \"Fair\": 5})\ndf[\"color\"] = df[\"color\"].map({\"D\": 1, \"E\": 2, \"F\": 3, \"G\": 4, \"H\": 5, \"I\": 6, \"J\": 7})\ndf[\"clarity\"] = df[\"clarity\"].map({\"IF\": 1, \"VVS1\": 2, \"VVS2\": 3, \"VS1\": 4, \"VS2\": 5, \"SI1\": 6, \"SI2\": 7, \"I1\": 8})\n\ndf.head()"
      ]
    },
    {
      "cell_type": "code",
      "id": "02eef7c1-8605-4fa9-a5ea-8ca51b98196e",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69d24eff-46bd-4e42-a005-368e65d44bd8",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Correlation Analysis\nCalculate the correlation coefficients between the features and the target variable (price). This will print the correlation coefficients of all features with the target variable in descending order. Positive values indicate a positive correlation, and negative values indicate a negative correlation."
      ]
    },
    {
      "cell_type": "code",
      "id": "a530cc49-b8db-477f-9bf1-0d503b17e8ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "correlation_matrix = df.corr()\ncorrelation_with_target = correlation_matrix['price'].sort_values(ascending=False)\nprint(correlation_with_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9b6daf4-823e-4792-b01e-b184ece1454a",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Visualize Correlations\nThis helps identify features with strong correlations (both positive and negative) with the target variable."
      ]
    },
    {
      "cell_type": "code",
      "id": "befb0bb0-28ac-4ffb-afbf-e778bd5898be",
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\nplt.title(\"Correlation Heatmap\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16f14a2a-bc16-4d1c-83c0-979271bceda5",
      "metadata": {},
      "outputs": [],
      "source": [
        "The above plot indicates that the impact of \"cut\", \"depth\", and \"table\" on price are relatively small. This could mean that the features don't have substantial effects on our predictions, so we can remove these features from out model:"
      ]
    },
    {
      "cell_type": "code",
      "id": "e811636d-15f3-498f-9941-34c0a09e69a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.drop([\"cut\", \"depth\", \"table\"], axis=1)\ndf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c56db5ff-5e4c-48cd-ae86-75f0acfa19e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Task 3: Feature Visualization "
      ]
    },
    {
      "cell_type": "code",
      "id": "418a2e29-38dc-4537-9aaf-f835070a56f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "p = sns.catplot(x='color', data=df , kind='count',aspect=2.5 )"
      ]
    },
    {
      "cell_type": "code",
      "id": "113c8ba1-2480-4ae2-ba53-5c98bcc81a7b",
      "metadata": {},
      "outputs": [],
      "source": [
        "p = sns.catplot(x='color', y='price', data=df, kind='box' ,aspect=2.5 )"
      ]
    },
    {
      "cell_type": "code",
      "id": "4ae74ec1-0a3a-4141-a5bd-785a0221cf28",
      "metadata": {},
      "outputs": [],
      "source": [
        "p = sns.catplot(x='clarity', data=df , kind='count',aspect=2.5 )"
      ]
    },
    {
      "cell_type": "code",
      "id": "8a5deb55-6416-4f71-9105-09cb7db5b8db",
      "metadata": {},
      "outputs": [],
      "source": [
        "p = sns.catplot(x='clarity', y='price', data=df, kind='box' ,aspect=2.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ef0b2a9-f53c-444d-8dfc-209950118586",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Task 4: Data Splits and Handle Outliers Using Isolation Forest"
      ]
    },
    {
      "cell_type": "code",
      "id": "a10aa048-a5d3-4819-9fff-0f0380ab4734",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into features (X) and target (y)\nX = df.drop([\"price\"], axis=1)\ny = df[\"price\"]\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                test_size=0.2, \n                                                random_state=42)\n\n# Check the shape of the training dataset\nprint(\"Shape of training data - X_train:\", X_train.shape)\nprint(\"Shape of training data - y_train:\", y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ab58d57-ea04-4f17-bffa-3434afc99a80",
      "metadata": {},
      "outputs": [],
      "source": [
        "Now we can use a a tree-based anomaly detection algorithm, Isolation Forest or iForest for short, to remove outliers automatically. It is based on modeling the normal data in such a way as to isolate anomalies that are both few in number and different in the feature space. "
      ]
    },
    {
      "cell_type": "code",
      "id": "35a7ef29-7de3-45ba-b101-a2425db1d299",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify outliers in the training dataset using Isolation Forest\niso = IsolationForest(contamination=0.1, random_state=42)\noutlier_predictions_train = iso.fit_predict(X_train)\n\n# Select non-outlier rows\nnon_outlier_mask = outlier_predictions_train != -1\nX_train, y_train = X_train.loc[non_outlier_mask], y_train.loc[non_outlier_mask]\n\n# Summarize the shape of the updated training dataset\nprint(\"Shape of updated training data - X_train:\", X_train.shape)\nprint(\"Shape of updated training data - y_train:\", y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5216bc5-bea0-4654-a381-503b8d4fd59a",
      "metadata": {},
      "outputs": [],
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e948bdd-8561-4c48-9e26-652f144720a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Build Models To Predict Diamond Prices\nIn this step, we will utilize a variety of machine learning algorithms, including 'Linear Regression', 'Lasso Regression', 'AdaBoost Regression', 'Ridge Regression', 'Gradient Boosting Regression', 'Random Forest Regression', and 'KNeighbours Regression'. Our aim is to train these models on our dataset and subsequently evaluate their performance. By doing so, we can identify the most suitable model for predicting diamond prices in the subsequent stages of our project.\n\n- **Linear Regression:** This algorithm establishes a linear relationship between the input features and the target variable, aiming to predict numeric values.\n\n- **Lasso Regression:** Similar to linear regression, this technique adds a penalty term to the model's cost function, encouraging the selection of only the most influential features and preventing overfitting.\n\n- **AdaBoost Regression:** This boosting algorithm constructs a strong predictive model by combining the outputs of several weak models in an iterative manner.\n\n- **Ridge Regression:** Like linear regression, ridge regression aims to predict outcomes. However, it introduces L2 regularization to manage multicollinearity and enhance model stability.\n\n- **Gradient Boosting Regression:** This ensemble method constructs a powerful model by sequentially refining weak learners, emphasizing previously misclassified instances.\n\n- **Random Forest Regression:** Another ensemble technique, random forest regression, creates multiple decision trees and aggregates their predictions to provide robust and accurate results.\n\n- **KNeighbours Regression:** This instance-based algorithm predicts outcomes by considering the outcomes of nearby data points in the feature space."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0247bd1-839e-418e-9e7c-5db0ea285fba",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Model Evaluation\n- **Mean Absolute Error (MAE):** The MAE represents the average absolute difference between the predicted values and the actual values. It can be calculated using the formula:\n```\nMAE = (1/n) * Î£ | y_i - Å·_i |\n```\n    Where:\n    - n is the number of data points.\n    - y_i is the actual value for the i-th data point.\n    - Å·_i is the predicted value for the i-th data point.\n    Lower MAE values indicate better performance.\n\n- **Mean Squared Error (MSE):** The MSE calculates the average squared difference between predicted and actual values:\n```\nMSE = (1/n) * Î£ (y_i - Å·_i)^2\n```\n    Where:\n    - n is the number of data points.\n    - y_i is the actual value for the i-th data point.\n    - Å·_i is the predicted value for the i-th data point.\n    Lower MSE values indicate better performance.\n\n- **R-squared Score (Coefficient of Determination)**: The R-squared score measures the proportion of variance predictable by the model:\n```\nR^2 = 1 - Î£ (y_i - Å·_i)^2 / Î£ (y_i - È³)^2\n```\n    Where:\n    - n is the number of data points.\n    - y_i is the actual value for the i-th data point.\n    - Å·_i is the predicted value for the i-th data point.\n    - È³ is the mean of the actual values.\n    Higher R^2 values indicate a better fit, ranging from 0 to 1."
      ]
    },
    {
      "cell_type": "code",
      "id": "9afd9d64-20f1-4c66-a03f-c3d2fc9c93a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all R2 scores\nR2_scores = []\nmodels = ['Linear Regression' , 'Lasso Regression' , 'AdaBoost Regression' , 'Ridge Regression' , 'Gradient Boosting Regression',\n          'Random Forest Regression' ,\n         'KNeighbours Regression']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b63ddb4-4355-40a0-9173-207e75cc4679",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "id": "fc6c8bf5-34a6-4200-b830-1e3da6597f1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and train the linear regression model\nclf_lr = LinearRegression()\nclf_lr.fit(X_train, y_train)\n\n# Predict diamond prices for the test set using the linear regression model\ny_pred = clf_lr.predict(X_test)\n\n# Calculate Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test, y_pred)\nprint(\"Mean Absolute Error: %.3f\" %mae)\n\n# Calculate Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error: %.3f\" %mse)\n\n# Calculate R-squared (Coefficient of Determination)\nr2 = r2_score(y_test, y_pred)\nprint(\"R-squared Score: %.3f\" %r2)\n\nR2_scores.append(r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "174964e8-4a0a-48de-b08f-0e7f7abc7a96",
      "metadata": {},
      "outputs": [],
      "source": [
        "In this case, an R-squared score of approximately 0.901 suggests that about 90% of the variance in diamond prices can be explained by the linear regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "525bdf6a-75bc-4a93-a59c-a1ad2bb7c5f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Lasso Regression "
      ]
    },
    {
      "cell_type": "code",
      "id": "e8e55a10-5d40-4a5c-aea0-045cae179699",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and train the Lasso regression model\nclf_la = Lasso()\nclf_la.fit(X_train , y_train)\n\n# Predict diamond prices for the test set using the linear regression model\ny_pred = clf_la.predict(X_test)\n\n\n# Calculate Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test, y_pred)\nprint(\"Mean Absolute Error: %.3f\" %mae)\n\n# Calculate Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error: %.3f\" %mse)\n\n# Calculate R-squared (Coefficient of Determination)\nr2 = r2_score(y_test, y_pred)\nprint(\"R-squared Score: %.3f\" %r2)\n\nR2_scores.append(r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a966ae8-d731-454d-a931-06a41a85e183",
      "metadata": {},
      "outputs": [],
      "source": [
        "In this case, an R-squared score of approximately 0.901 suggests that about 90% of the variance in diamond prices can be explained by the Lasso regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b52f8d9-736f-40f4-8cd9-4e20bbf22a0b",
      "metadata": {},
      "outputs": [],
      "source": [
        "## AdaBoost Regression"
      ]
    },
    {
      "cell_type": "code",
      "id": "d36d963b-8cc1-4b43-8579-bf922cd6cef1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and train the AdaBoost regression model\nclf_ar = AdaBoostRegressor()\nclf_ar.fit(X_train , y_train)\n\n# Predict diamond prices for the test set using the linear regression model\ny_pred = clf_ar.predict(X_test)\n\n# Calculate Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test, y_pred)\nprint(\"Mean Absolute Error: %.3f\" %mae)\n\n# Calculate Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error: %.3f\" %mse)\n\n# Calculate R-squared (Coefficient of Determination)\nr2 = r2_score(y_test, y_pred)\nprint(\"R-squared Score: %.3f\" %r2)\n\nR2_scores.append(r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9de2d65b-1609-44fd-a050-136c2813f3e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "In this case, an R-squared score of approximately 0.934 suggests that about 93% of the variance in diamond prices can be explained by the AdaBoost regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bb22818-4a47-4359-a5e9-f6d0f9b5a797",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Ridge Regression "
      ]
    },
    {
      "cell_type": "code",
      "id": "38478645-8837-4694-b79b-83d16914b08a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and train the Ridge regression model\nclf_rr = Ridge()\nclf_rr.fit(X_train , y_train)\n\n# Predict diamond prices for the test set using the linear regression model\ny_pred = clf_rr.predict(X_test)\n\n# Calculate Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test, y_pred)\nprint(\"Mean Absolute Error: %.3f\" %mae)\n\n# Calculate Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error: %.3f\" %mse)\n\n# Calculate R-squared (Coefficient of Determination)\nr2 = r2_score(y_test, y_pred)\nprint(\"R-squared Score: %.3f\" %r2)\n\nR2_scores.append(r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24c6bc26-e025-4cde-9c66-89d997a2803e",
      "metadata": {},
      "outputs": [],
      "source": [
        "In this case, an R-squared score of approximately 0.901 suggests that about 90% of the variance in diamond prices can be explained by the Ridge regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02278c42-dd72-4a57-bf35-be2f52d90bfd",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Gradient Boosting Regression"
      ]
    },
    {
      "cell_type": "code",
      "id": "e33036a9-d6cb-4c7e-9bcb-ebc5ff203348",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and train the GradientBoosting regression model\nclf_gbr = GradientBoostingRegressor()\nclf_gbr.fit(X_train , y_train)\n\n# Predict diamond prices for the test set using the linear regression model\ny_pred = clf_gbr.predict(X_test)\n\n# Calculate Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test, y_pred)\nprint(\"Mean Absolute Error: %.3f\" %mae)\n\n# Calculate Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error: %.3f\" %mse)\n\n# Calculate R-squared (Coefficient of Determination)\nr2 = r2_score(y_test, y_pred)\nprint(\"R-squared Score: %.3f\" %r2)\n\nR2_scores.append(r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6625fdd-8f2a-4429-9a19-3d7c37cb9a3f",
      "metadata": {},
      "outputs": [],
      "source": [
        "In this case, an R-squared score of approximately 0.965 suggests that about 97% of the variance in diamond prices can be explained by the Gradient Boosting regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7692762b-3ef3-4528-93c7-ccd98b5aa03b",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Random Forest Regression "
      ]
    },
    {
      "cell_type": "code",
      "id": "3a0e071b-29c1-468d-8bb7-eacaf40f15c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and train the Random Forest regression model\nclf_rf = RandomForestRegressor()\nclf_rf.fit(X_train , y_train)\n\n# Predict diamond prices for the test set using the linear regression model\ny_pred = clf_rf.predict(X_test)\n\n# Calculate Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test, y_pred)\nprint(\"Mean Absolute Error: %.3f\" %mae)\n\n# Calculate Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error: %.3f\" %mse)\n\n# Calculate R-squared (Coefficient of Determination)\nr2 = r2_score(y_test, y_pred)\nprint(\"R-squared Score: %.3f\" %r2)\n\nR2_scores.append(r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd058a78-604c-42fa-acf9-482ddadbcd6d",
      "metadata": {},
      "outputs": [],
      "source": [
        "In this case, an R-squared score of approximately 0.962 suggests that about 96% of the variance in diamond prices can be explained by the Random Forest regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25761eb2-4e58-4d0d-afd3-74005246b87e",
      "metadata": {},
      "outputs": [],
      "source": [
        "## KNeighbours Regression "
      ]
    },
    {
      "cell_type": "code",
      "id": "36eafa41-e231-47cf-b76a-f3e4665b6014",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and train the KNeighbours regression model\nclf_knn = KNeighborsRegressor()\nclf_knn.fit(X_train , y_train)\n\n# Predict diamond prices for the test set using the linear regression model\ny_pred = clf_knn.predict(X_test)\n\n# Calculate Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test, y_pred)\nprint(\"Mean Absolute Error: %.3f\" %mae)\n\n# Calculate Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error: %.3f\" %mse)\n\n# Calculate R-squared (Coefficient of Determination)\nr2 = r2_score(y_test, y_pred)\nprint(\"R-squared Score: %.3f\" %r2)\n\nR2_scores.append(r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f32713de-cf61-46e1-8bfb-a90704b2a1ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "In this case, an R-squared score of approximately 0.867 suggests that about 87% of the variance in diamond prices can be explained by the KNeighbours regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9de6bab-d15a-474c-9a87-d3c27866aca5",
      "metadata": {},
      "outputs": [],
      "source": [
        "## R2-Score Visualization"
      ]
    },
    {
      "cell_type": "code",
      "id": "cb72c878-446e-4433-9f50-a35184b30623",
      "metadata": {},
      "outputs": [],
      "source": [
        "compare = pd.DataFrame({'Algorithms' : models , 'R2 Scores' : R2_scores})\ncompare.sort_values(by='R2 Scores' ,ascending=False).style.background_gradient(cmap=\"Reds\")"
      ]
    },
    {
      "cell_type": "code",
      "id": "3ab9d3f6-a192-44ee-b32d-83db115e6dc5",
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.catplot(x='Algorithms', y='R2 Scores' , data=compare, kind='point', aspect=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "572766e5-f179-4af2-b7ac-11a8602053d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "Since the Gradient Boosting Algorithm has achieved the highest R2 score among all the models, we will exclusively employ this model for the subsequent diamond price prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76da67ba-d902-4dd4-bf94-4b1c69796e07",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Diamond Price Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c398b759-ddb8-4e8a-b13b-f101d327cda6",
      "metadata": {},
      "outputs": [],
      "source": [
        "You can use the following code to make predictions about diamond prices based on user input:\n\n```\na = float(input(\"Carat Size: \"))\nb = int(input(\"Color (1-7): \"))\nc = int(input(\"Clarity (1-8): \"))\nfeatures = np.array([[a, b, c]])\nprint(\"Predicted Diamond's Price = \", clf_gbr.predict(features))\n```\n\nDue to the fact that Mercury Cloud does not display user inputs, running this code within the app on Mercury Cloud could potentially lead to errors. Hence, it is advisable not to execute this code within the notebook uploaded to Mercury."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1653246-f4be-4627-b446-a43c77b653b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aff86a33-5422-43cb-a4c6-31302ec6b10e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Deploy the Notebook as a Web App Using Mercury\nThe final step is to deploy our model as an interactive web app using open-source framework, Mercury. This will allow users to input diamond attributes and receive instant price predictions.\n\n## Understanding Mercury\n### 1. What is Mercury?\nMercury Server is the core of our web app framework. It's built using technologies like Django, Django Channels, and React. Imagine it as a bridge between our notebooks and the web browser. When a user opens a notebook in a web browser, Mercury Server steps in and establishes a connection, much like a virtual highway, between the browser and itself.\n\n### 2. How It Works:\n\n- **WebSocket Connection:** This connection is established between the user's web browser and Mercury Server. It's like a live channel of communication that lets data flow back and forth seamlessly.\n- **Mercury Server and Worker:** Mercury Server ensures there's a \"worker\" ready to handle requests. Think of this worker as a skilled assistant. It connects to Mercury Server via WebSocket.\n- **Action Forwarding:** When a user interacts with the app in their browser, every action they take is forwarded by Mercury Server to the worker. It's like passing notes between them.\n\n- **Kernel Magic:** The worker maintains an open IPython kernel and understands the code from our notebook. It's our app's brain. When users interact with widgets, the worker's kernel executes the code related to those interactions.\n\n- **Results Flow:** The worker sends the results of these executions back to the user's browser through Mercury Server. Imagine getting a quick answer after asking a question.\n\n### 3. Why Mercury?\nMercury is the simplest way to transform our notebooks into web apps. It offers great features:\n- You can show or hide your code.\n- Your users can easily export executed notebook to PDF/HTML.\n- There is built-in authentication, so you can control who accesses your app.\n- You can produce files in the notebook and make them downloadable.\n- You can share multiple notebooks with Mercury Cloud."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "427f7b7f-8b13-4443-b9bb-d0bb8423d719",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Task 0: Getting Started with Web App Transformation\nTo kickstart the process of turning your Jupyter Notebook into an interactive web app, make sure to retain the following code. <font color=\"red\">There's no need to execute the code as it's incompatible with the notebook version. However, it will serve as a foundational element for the upcoming steps.</font>\n\n## Task 1: Download the Notebook\nLook for the \"Download Notebook\" button at the top of the notebook's toolbar to download the notebook.\n\n## Task 2: Setup a Mercury Account\n1. Sign up for a free account at [Mercury](https://cloud.runmercury.com/register).\n2. After signing up, log in to your Mercury account.\n\n## Task 3: Create Your First Site\n1. Look for the green **\"+ Add Site\"** button and click it.\n2. In the \"Title of your website\" field, enter `Diamond Price Prediction`.\n3. In the \"Subdomain at which website will be available\" field, enter `demo`(Please note that site subdomain names must be unique. In case the chosen subdomain name is already taken, be prepared to modify it accordingly).\n4. Click the green **\"OK\"** button at the bottom of the page.\n\n## Task 4: Upload Your File\n1. Click the **\"Upload Files\"** button on the right side of the site you just created.\n2. Upload your downloaded `ipynb` notebook from Task 1 into your new site. \n\n## Task 5: Open the App\n1. Return to the sites menu by clicking the **\"Sites\"** button on the top navigation bar.\n2. Click the link associated with the `Diamond Price Prediction` site.\n3. VoilÃ ! You've successfully accessed your site where you can explore and interact with your uploaded notebook."
      ]
    },
    {
      "cell_type": "code",
      "id": "38e3eb5a-9822-4119-8012-3e40bf3bfea6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import mercury as mr\nimport math\n\n# Configure the app:\n# The `App` class controls how the notebook is displayed in the Mercury.\napp = mr.App(title=\"ðŸ’Ž Diamond Price Prediction\",\n        description=\"\",\n        show_code=False)\n\n# Populate the app with widgets:\n# Use the `mr.Note` widget to display a Markdown text. It's used here to prompt the user to select a metric using the slider.\nmr.Note(text=\"__Select the metrics to see the prediction__\")\n\n# Calculate the maximum and minimum values of the feature columns in the `DataFrame`. These values will define the range of the sliders.\nc1_max = df['carat'].max()\nc1_min = df['carat'].min()\n# a slider widget capable of handling decimal numbers\ncarat = mr.Numeric(label=\"Carat\", value=c1_min, min=c1_min, max=c1_max, step=0.1)\n\nc2_max = df['color'].max()\nc2_min = df['color'].min()\n# a slider widget capable of handling whole numbers\ncolor = mr.Slider(label=\"Color\", value=c2_min, min=c2_min, max=c2_max)\n\nc3_max = df['clarity'].max()\nc3_min = df['clarity'].min()\nclarity = mr.Slider(label=\"Clarity\", value=c3_min, min=c3_min, max=c3_max)"
      ]
    },
    {
      "cell_type": "code",
      "id": "57e8dc46-7b61-460a-aae4-d464875b329d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# display numbers in large boxes with title\nmr.NumberBox(data=\"Predicted Price in USD: $\" + str(math.ceil((float(clf_gbr.predict(np.array([[carat.value, color.value, clarity.value]])))))))"
      ]
    },
    {
      "cell_type": "code",
      "id": "e1de3f34-007e-4ec2-90f1-efe232bd22a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# display numbers in a row of three boxes\nmr.NumberBox([\n    mr.NumberBox(data=carat.value, title=\"Carat\"),\n    mr.NumberBox(data=color.value, title=\"Color\"),\n    mr.NumberBox(data=clarity.value, title=\"Clarity\")\n])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b6ee5d1-80c2-43f0-9d65-81f467a2f7c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9f89417-9675-4700-b4a7-fc85337d519e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Congratulations! You have completed the project"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28c6b9fd-a3aa-425b-991c-967b316e3e66",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Authors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f59cdee-29d3-4852-8f0c-80324669c1e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "[Vicky Kuo](https://author.skills.network/instructors/vicky_kuo)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d381b3f3-07ca-4e4a-ba7c-f8cd2925b9bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Change Log"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "812f95ee-3a8d-4404-a859-f49fec97e451",
      "metadata": {},
      "outputs": [],
      "source": [
        "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n|-|-|-|-|\n|2023-05-01|0.1|Vicky Kuo|Initial Lab Created|"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6310602c-1fab-4b8f-87b4-3f4d619f9df2",
      "metadata": {},
      "outputs": [],
      "source": [
        "Copyright Â© 2023 IBM Corporation. All rights reserved."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": ""
    },
    "kernelspec": {
      "name": "",
      "display_name": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}